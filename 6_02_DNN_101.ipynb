{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/constantinouelenios-IB2AD0/constantinouelenios-IB2AD0-DataScience-GenAI-Submissions/blob/main/6_02_DNN_101.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "![](https://drive.google.com/uc?export=view&id=1xqQczl0FG-qtNA2_WQYuWePW9oU8irqJ)"
      ],
      "metadata": {
        "id": "E0T9_-jFXxxc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 6.02 Dense Neural Network (with PyTorch)\n",
        "This will expand on our logistic regression example and take us through building our first neural network. If you haven't already, be sure to check (and if neccessary) switch to GPU processing by clicking Runtime > Change runtime type and selecting GPU. We can test this has worked with the following code:"
      ],
      "metadata": {
        "id": "dcEWDwlu94Xs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "\n",
        "# Check for GPU availability\n",
        "print(\"Num GPUs Available: \", torch.cuda.device_count())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "v8cIpNbCvuQA",
        "outputId": "929f0a89-c9f2-4167-d4b8-540fe1923f1f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Num GPUs Available:  1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Hopefully your code shows you have 1 GPU available! Next let's get some data. We'll start with another in-built dataset:"
      ],
      "metadata": {
        "id": "8d6FF1wK-ph8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# upload an in-built Python (OK semi-in-built) dataset\n",
        "from sklearn.datasets import load_diabetes\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "# import the data\n",
        "data = load_diabetes()\n",
        "data"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2MziWWXu-0ur",
        "outputId": "35010abf-fe5f-4d18-8226-5ef1dddfe693"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'data': array([[ 0.03807591,  0.05068012,  0.06169621, ..., -0.00259226,\n",
              "          0.01990749, -0.01764613],\n",
              "        [-0.00188202, -0.04464164, -0.05147406, ..., -0.03949338,\n",
              "         -0.06833155, -0.09220405],\n",
              "        [ 0.08529891,  0.05068012,  0.04445121, ..., -0.00259226,\n",
              "          0.00286131, -0.02593034],\n",
              "        ...,\n",
              "        [ 0.04170844,  0.05068012, -0.01590626, ..., -0.01107952,\n",
              "         -0.04688253,  0.01549073],\n",
              "        [-0.04547248, -0.04464164,  0.03906215, ...,  0.02655962,\n",
              "          0.04452873, -0.02593034],\n",
              "        [-0.04547248, -0.04464164, -0.0730303 , ..., -0.03949338,\n",
              "         -0.00422151,  0.00306441]]),\n",
              " 'target': array([151.,  75., 141., 206., 135.,  97., 138.,  63., 110., 310., 101.,\n",
              "         69., 179., 185., 118., 171., 166., 144.,  97., 168.,  68.,  49.,\n",
              "         68., 245., 184., 202., 137.,  85., 131., 283., 129.,  59., 341.,\n",
              "         87.,  65., 102., 265., 276., 252.,  90., 100.,  55.,  61.,  92.,\n",
              "        259.,  53., 190., 142.,  75., 142., 155., 225.,  59., 104., 182.,\n",
              "        128.,  52.,  37., 170., 170.,  61., 144.,  52., 128.,  71., 163.,\n",
              "        150.,  97., 160., 178.,  48., 270., 202., 111.,  85.,  42., 170.,\n",
              "        200., 252., 113., 143.,  51.,  52., 210.,  65., 141.,  55., 134.,\n",
              "         42., 111.,  98., 164.,  48.,  96.,  90., 162., 150., 279.,  92.,\n",
              "         83., 128., 102., 302., 198.,  95.,  53., 134., 144., 232.,  81.,\n",
              "        104.,  59., 246., 297., 258., 229., 275., 281., 179., 200., 200.,\n",
              "        173., 180.,  84., 121., 161.,  99., 109., 115., 268., 274., 158.,\n",
              "        107.,  83., 103., 272.,  85., 280., 336., 281., 118., 317., 235.,\n",
              "         60., 174., 259., 178., 128.,  96., 126., 288.,  88., 292.,  71.,\n",
              "        197., 186.,  25.,  84.,  96., 195.,  53., 217., 172., 131., 214.,\n",
              "         59.,  70., 220., 268., 152.,  47.,  74., 295., 101., 151., 127.,\n",
              "        237., 225.,  81., 151., 107.,  64., 138., 185., 265., 101., 137.,\n",
              "        143., 141.,  79., 292., 178.,  91., 116.,  86., 122.,  72., 129.,\n",
              "        142.,  90., 158.,  39., 196., 222., 277.,  99., 196., 202., 155.,\n",
              "         77., 191.,  70.,  73.,  49.,  65., 263., 248., 296., 214., 185.,\n",
              "         78.,  93., 252., 150.,  77., 208.,  77., 108., 160.,  53., 220.,\n",
              "        154., 259.,  90., 246., 124.,  67.,  72., 257., 262., 275., 177.,\n",
              "         71.,  47., 187., 125.,  78.,  51., 258., 215., 303., 243.,  91.,\n",
              "        150., 310., 153., 346.,  63.,  89.,  50.,  39., 103., 308., 116.,\n",
              "        145.,  74.,  45., 115., 264.,  87., 202., 127., 182., 241.,  66.,\n",
              "         94., 283.,  64., 102., 200., 265.,  94., 230., 181., 156., 233.,\n",
              "         60., 219.,  80.,  68., 332., 248.,  84., 200.,  55.,  85.,  89.,\n",
              "         31., 129.,  83., 275.,  65., 198., 236., 253., 124.,  44., 172.,\n",
              "        114., 142., 109., 180., 144., 163., 147.,  97., 220., 190., 109.,\n",
              "        191., 122., 230., 242., 248., 249., 192., 131., 237.,  78., 135.,\n",
              "        244., 199., 270., 164.,  72.,  96., 306.,  91., 214.,  95., 216.,\n",
              "        263., 178., 113., 200., 139., 139.,  88., 148.,  88., 243.,  71.,\n",
              "         77., 109., 272.,  60.,  54., 221.,  90., 311., 281., 182., 321.,\n",
              "         58., 262., 206., 233., 242., 123., 167.,  63., 197.,  71., 168.,\n",
              "        140., 217., 121., 235., 245.,  40.,  52., 104., 132.,  88.,  69.,\n",
              "        219.,  72., 201., 110.,  51., 277.,  63., 118.,  69., 273., 258.,\n",
              "         43., 198., 242., 232., 175.,  93., 168., 275., 293., 281.,  72.,\n",
              "        140., 189., 181., 209., 136., 261., 113., 131., 174., 257.,  55.,\n",
              "         84.,  42., 146., 212., 233.,  91., 111., 152., 120.,  67., 310.,\n",
              "         94., 183.,  66., 173.,  72.,  49.,  64.,  48., 178., 104., 132.,\n",
              "        220.,  57.]),\n",
              " 'frame': None,\n",
              " 'DESCR': '.. _diabetes_dataset:\\n\\nDiabetes dataset\\n----------------\\n\\nTen baseline variables, age, sex, body mass index, average blood\\npressure, and six blood serum measurements were obtained for each of n =\\n442 diabetes patients, as well as the response of interest, a\\nquantitative measure of disease progression one year after baseline.\\n\\n**Data Set Characteristics:**\\n\\n:Number of Instances: 442\\n\\n:Number of Attributes: First 10 columns are numeric predictive values\\n\\n:Target: Column 11 is a quantitative measure of disease progression one year after baseline\\n\\n:Attribute Information:\\n    - age     age in years\\n    - sex\\n    - bmi     body mass index\\n    - bp      average blood pressure\\n    - s1      tc, total serum cholesterol\\n    - s2      ldl, low-density lipoproteins\\n    - s3      hdl, high-density lipoproteins\\n    - s4      tch, total cholesterol / HDL\\n    - s5      ltg, possibly log of serum triglycerides level\\n    - s6      glu, blood sugar level\\n\\nNote: Each of these 10 feature variables have been mean centered and scaled by the standard deviation times the square root of `n_samples` (i.e. the sum of squares of each column totals 1).\\n\\nSource URL:\\nhttps://www4.stat.ncsu.edu/~boos/var.select/diabetes.html\\n\\nFor more information see:\\nBradley Efron, Trevor Hastie, Iain Johnstone and Robert Tibshirani (2004) \"Least Angle Regression,\" Annals of Statistics (with discussion), 407-499.\\n(https://web.stanford.edu/~hastie/Papers/LARS/LeastAngle_2002.pdf)\\n',\n",
              " 'feature_names': ['age',\n",
              "  'sex',\n",
              "  'bmi',\n",
              "  'bp',\n",
              "  's1',\n",
              "  's2',\n",
              "  's3',\n",
              "  's4',\n",
              "  's5',\n",
              "  's6'],\n",
              " 'data_filename': 'diabetes_data_raw.csv.gz',\n",
              " 'target_filename': 'diabetes_target.csv.gz',\n",
              " 'data_module': 'sklearn.datasets.data'}"
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "We are working on a regression problem, with \"structured\" data which has already been cleaned and normalised. We can skip the usual cleaning/engineering steps. However, we do need to get the data into PyTorch:"
      ],
      "metadata": {
        "id": "cZKrbx70_cIT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Convert data to PyTorch tensors\n",
        "X = torch.tensor(data.data, dtype=torch.float32)\n",
        "y = torch.tensor(data.target, dtype=torch.float32).reshape(-1, 1) # Reshape y to be a column vector"
      ],
      "metadata": {
        "id": "f9PHiljr73fI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now our data is stored in tensors we can do train/test splitting as before (in fact we can use sklearn as before):"
      ],
      "metadata": {
        "id": "hu8VH2_SAOoj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "print(X_train.shape, y_train.shape)\n",
        "print(X_test.shape, y_test.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PYJN01DV8Fac",
        "outputId": "788d3031-84d8-4bc8-c1e0-b860492b3c54"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([353, 10]) torch.Size([353, 1])\n",
            "torch.Size([89, 10]) torch.Size([89, 1])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now we can set up our batches for training. As we have a nice round 400 let's go with batches of 50 (8 batches in total). We'll also seperate the features and labels:"
      ],
      "metadata": {
        "id": "LKmbZoCrJijU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.utils.data import TensorDataset, DataLoader\n",
        "\n",
        "# Create TensorDatasets and DataLoaders\n",
        "train_dataset = TensorDataset(X_train, y_train)\n",
        "train_loader = DataLoader(train_dataset, batch_size=50, shuffle=True)\n",
        "\n",
        "test_dataset = TensorDataset(X_test, y_test)\n",
        "test_loader = DataLoader(test_dataset, batch_size=50, shuffle=False)"
      ],
      "metadata": {
        "id": "de0uOko08d-K"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now its time to build our model. We'll keep it simple ... a model with an input layer of 10 features and then 2x _Dense_ (fully connected) layers each with 5 neurons and ReLU activation. Our output layer will be size=1 given this is a regression problem and we want a single value output per prediction.\n",
        "\n",
        "This will be easier to understand if you have read through the logistic regression tutorial."
      ],
      "metadata": {
        "id": "yCCG8kKHCVnd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "# Define the model\n",
        "class DiabetesModel(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(DiabetesModel, self).__init__()\n",
        "        # we'll set up the layers as a sequence using nn.Sequential\n",
        "        self.layers = nn.Sequential(\n",
        "\n",
        "            # first layer will be a linear layer that has 5x neurons\n",
        "            # (5x sets of linear regression)\n",
        "            # the layer takes the 10 features as input (i.e. 10, 5)\n",
        "            nn.Linear(10, 5),\n",
        "\n",
        "            nn.ReLU(), # ReLU activation\n",
        "\n",
        "            # second linear layer again has 5 neurons\n",
        "            # this time taking the input as the output of the last layer\n",
        "            # (which had 5x neurons)\n",
        "            nn.Linear(5, 5),\n",
        "\n",
        "            nn.ReLU(), # ReLU again\n",
        "\n",
        "            # last linear layer takes the output from the previous 5 neurons\n",
        "            # this time its a single output with no activation\n",
        "            # i.e. this is the predicitons (regression)\n",
        "            nn.Linear(5, 1)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.layers(x) # pass the data through the layers"
      ],
      "metadata": {
        "id": "844H60hcCV3s"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "As before we need to create a model object, specify the loss (criterion) and an optimiser (which we cover next week):"
      ],
      "metadata": {
        "id": "cv4-loCz91aa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.optim as optim\n",
        "\n",
        "# Initialize the model, loss function, and optimizer\n",
        "model = DiabetesModel()\n",
        "criterion = nn.MSELoss() # MSE loss function\n",
        "optimiser = optim.Adam(model.parameters(), lr=0.001)"
      ],
      "metadata": {
        "id": "EPx_Wy6g9uA4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now we can train the model. Again, the logistic regression tutorial (6.01) may help you undertstand this:"
      ],
      "metadata": {
        "id": "HOKfjkfW-Ish"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "\n",
        "# Move model to GPU if available\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# Training loop (example - you'll likely want to add more epochs)\n",
        "epochs = 1000 # 100 epochs\n",
        "\n",
        "for epoch in range(epochs):\n",
        "  # use the train_loader to pass the inputs (x) and targets (y)\n",
        "  for inputs, targets in train_loader:\n",
        "    # pass to the GPU (hopefully)\n",
        "    inputs, targets = inputs.to(device), targets.to(device)\n",
        "\n",
        "    # pass model to GPU as well\n",
        "    model.to(device)\n",
        "\n",
        "    model.train() # put the model object in train mode\n",
        "    optimiser.zero_grad() # reset the gradiants\n",
        "    outputs = model(inputs) # create outputs\n",
        "    loss = criterion(outputs, targets) # compare with Y to get loss\n",
        "    loss.backward() # backpropogate the loss (next week)\n",
        "    optimiser.step() # # update the parameters based on this round of training\n",
        "\n",
        "  # every 10 steps we will print out the current loss\n",
        "    if (epoch+1) % 10 == 0: # modular arithmetic\n",
        "        print(f'Epoch [{epoch+1}/{epochs}], Loss: {round(loss.item(), 4)}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PtMUgfwT-HGt",
        "outputId": "98a55275-5a95-4942-aec0-8a2419167301"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [10/1000], Loss: 18397.2188\n",
            "Epoch [10/1000], Loss: 18960.2793\n",
            "Epoch [10/1000], Loss: 19095.1152\n",
            "Epoch [10/1000], Loss: 18774.9551\n",
            "Epoch [10/1000], Loss: 16317.1494\n",
            "Epoch [10/1000], Loss: 18642.7891\n",
            "Epoch [10/1000], Loss: 18662.4238\n",
            "Epoch [10/1000], Loss: 5852.3555\n",
            "Epoch [20/1000], Loss: 15927.1299\n",
            "Epoch [20/1000], Loss: 18052.4375\n",
            "Epoch [20/1000], Loss: 13231.3506\n",
            "Epoch [20/1000], Loss: 16848.5938\n",
            "Epoch [20/1000], Loss: 15943.2119\n",
            "Epoch [20/1000], Loss: 18242.4941\n",
            "Epoch [20/1000], Loss: 14385.4951\n",
            "Epoch [20/1000], Loss: 16608.6934\n",
            "Epoch [30/1000], Loss: 15311.8643\n",
            "Epoch [30/1000], Loss: 18082.7988\n",
            "Epoch [30/1000], Loss: 15135.8145\n",
            "Epoch [30/1000], Loss: 14106.8242\n",
            "Epoch [30/1000], Loss: 10965.0752\n",
            "Epoch [30/1000], Loss: 10892.9482\n",
            "Epoch [30/1000], Loss: 12660.0283\n",
            "Epoch [30/1000], Loss: 10401.6436\n",
            "Epoch [40/1000], Loss: 9548.0723\n",
            "Epoch [40/1000], Loss: 9737.2031\n",
            "Epoch [40/1000], Loss: 13261.9805\n",
            "Epoch [40/1000], Loss: 11411.7539\n",
            "Epoch [40/1000], Loss: 13792.4482\n",
            "Epoch [40/1000], Loss: 13869.5547\n",
            "Epoch [40/1000], Loss: 10700.502\n",
            "Epoch [40/1000], Loss: 15913.5469\n",
            "Epoch [50/1000], Loss: 9786.7812\n",
            "Epoch [50/1000], Loss: 8379.4512\n",
            "Epoch [50/1000], Loss: 8080.02\n",
            "Epoch [50/1000], Loss: 10478.8496\n",
            "Epoch [50/1000], Loss: 9668.4434\n",
            "Epoch [50/1000], Loss: 10891.7773\n",
            "Epoch [50/1000], Loss: 12103.8721\n",
            "Epoch [50/1000], Loss: 12877.0781\n",
            "Epoch [60/1000], Loss: 7736.3442\n",
            "Epoch [60/1000], Loss: 6672.4849\n",
            "Epoch [60/1000], Loss: 9251.7285\n",
            "Epoch [60/1000], Loss: 10374.9854\n",
            "Epoch [60/1000], Loss: 8950.2451\n",
            "Epoch [60/1000], Loss: 7515.6899\n",
            "Epoch [60/1000], Loss: 7836.3125\n",
            "Epoch [60/1000], Loss: 680.5134\n",
            "Epoch [70/1000], Loss: 5664.6875\n",
            "Epoch [70/1000], Loss: 5525.9272\n",
            "Epoch [70/1000], Loss: 9668.6104\n",
            "Epoch [70/1000], Loss: 6082.3994\n",
            "Epoch [70/1000], Loss: 6286.2407\n",
            "Epoch [70/1000], Loss: 7231.4561\n",
            "Epoch [70/1000], Loss: 8133.5854\n",
            "Epoch [70/1000], Loss: 14870.1855\n",
            "Epoch [80/1000], Loss: 6018.6943\n",
            "Epoch [80/1000], Loss: 6944.7188\n",
            "Epoch [80/1000], Loss: 5418.9663\n",
            "Epoch [80/1000], Loss: 6139.043\n",
            "Epoch [80/1000], Loss: 4956.8418\n",
            "Epoch [80/1000], Loss: 5787.3716\n",
            "Epoch [80/1000], Loss: 6766.5137\n",
            "Epoch [80/1000], Loss: 8736.7617\n",
            "Epoch [90/1000], Loss: 4848.2749\n",
            "Epoch [90/1000], Loss: 4434.4741\n",
            "Epoch [90/1000], Loss: 5859.4834\n",
            "Epoch [90/1000], Loss: 5430.5488\n",
            "Epoch [90/1000], Loss: 5421.5967\n",
            "Epoch [90/1000], Loss: 5818.4834\n",
            "Epoch [90/1000], Loss: 4826.7896\n",
            "Epoch [90/1000], Loss: 12730.5459\n",
            "Epoch [100/1000], Loss: 4469.4966\n",
            "Epoch [100/1000], Loss: 4682.0854\n",
            "Epoch [100/1000], Loss: 6305.8271\n",
            "Epoch [100/1000], Loss: 3522.0852\n",
            "Epoch [100/1000], Loss: 5544.1562\n",
            "Epoch [100/1000], Loss: 4847.0029\n",
            "Epoch [100/1000], Loss: 3979.3679\n",
            "Epoch [100/1000], Loss: 7819.2539\n",
            "Epoch [110/1000], Loss: 4691.6982\n",
            "Epoch [110/1000], Loss: 4693.8457\n",
            "Epoch [110/1000], Loss: 4830.2876\n",
            "Epoch [110/1000], Loss: 4633.7656\n",
            "Epoch [110/1000], Loss: 4094.7493\n",
            "Epoch [110/1000], Loss: 4661.769\n",
            "Epoch [110/1000], Loss: 3600.0061\n",
            "Epoch [110/1000], Loss: 9277.8564\n",
            "Epoch [120/1000], Loss: 4916.874\n",
            "Epoch [120/1000], Loss: 3778.7009\n",
            "Epoch [120/1000], Loss: 3810.8877\n",
            "Epoch [120/1000], Loss: 5645.9556\n",
            "Epoch [120/1000], Loss: 4494.8838\n",
            "Epoch [120/1000], Loss: 3807.5793\n",
            "Epoch [120/1000], Loss: 3803.1699\n",
            "Epoch [120/1000], Loss: 3497.9597\n",
            "Epoch [130/1000], Loss: 4990.937\n",
            "Epoch [130/1000], Loss: 3781.7605\n",
            "Epoch [130/1000], Loss: 3456.9724\n",
            "Epoch [130/1000], Loss: 3037.6333\n",
            "Epoch [130/1000], Loss: 3258.9812\n",
            "Epoch [130/1000], Loss: 5892.8784\n",
            "Epoch [130/1000], Loss: 5127.4497\n",
            "Epoch [130/1000], Loss: 2555.8103\n",
            "Epoch [140/1000], Loss: 3947.9817\n",
            "Epoch [140/1000], Loss: 4891.2866\n",
            "Epoch [140/1000], Loss: 3783.1611\n",
            "Epoch [140/1000], Loss: 4139.8735\n",
            "Epoch [140/1000], Loss: 4047.6184\n",
            "Epoch [140/1000], Loss: 3442.6494\n",
            "Epoch [140/1000], Loss: 4770.897\n",
            "Epoch [140/1000], Loss: 3080.7979\n",
            "Epoch [150/1000], Loss: 2964.2651\n",
            "Epoch [150/1000], Loss: 4001.7156\n",
            "Epoch [150/1000], Loss: 4673.1436\n",
            "Epoch [150/1000], Loss: 4207.8403\n",
            "Epoch [150/1000], Loss: 4953.6841\n",
            "Epoch [150/1000], Loss: 3440.3655\n",
            "Epoch [150/1000], Loss: 4414.4399\n",
            "Epoch [150/1000], Loss: 2165.9937\n",
            "Epoch [160/1000], Loss: 3440.0386\n",
            "Epoch [160/1000], Loss: 4119.4199\n",
            "Epoch [160/1000], Loss: 3377.0134\n",
            "Epoch [160/1000], Loss: 4042.9558\n",
            "Epoch [160/1000], Loss: 3710.7886\n",
            "Epoch [160/1000], Loss: 4546.9331\n",
            "Epoch [160/1000], Loss: 4742.1367\n",
            "Epoch [160/1000], Loss: 8835.7461\n",
            "Epoch [170/1000], Loss: 4482.5225\n",
            "Epoch [170/1000], Loss: 3376.6206\n",
            "Epoch [170/1000], Loss: 3552.0491\n",
            "Epoch [170/1000], Loss: 4660.4092\n",
            "Epoch [170/1000], Loss: 4886.6035\n",
            "Epoch [170/1000], Loss: 3543.4832\n",
            "Epoch [170/1000], Loss: 3661.2612\n",
            "Epoch [170/1000], Loss: 1291.5066\n",
            "Epoch [180/1000], Loss: 3990.8281\n",
            "Epoch [180/1000], Loss: 3690.6506\n",
            "Epoch [180/1000], Loss: 4324.5718\n",
            "Epoch [180/1000], Loss: 3261.9644\n",
            "Epoch [180/1000], Loss: 3431.7126\n",
            "Epoch [180/1000], Loss: 4441.0537\n",
            "Epoch [180/1000], Loss: 4550.73\n",
            "Epoch [180/1000], Loss: 5268.4941\n",
            "Epoch [190/1000], Loss: 4440.1636\n",
            "Epoch [190/1000], Loss: 3778.5962\n",
            "Epoch [190/1000], Loss: 3939.5247\n",
            "Epoch [190/1000], Loss: 3315.9146\n",
            "Epoch [190/1000], Loss: 4704.0654\n",
            "Epoch [190/1000], Loss: 3758.571\n",
            "Epoch [190/1000], Loss: 3567.1208\n",
            "Epoch [190/1000], Loss: 4324.3633\n",
            "Epoch [200/1000], Loss: 4028.4062\n",
            "Epoch [200/1000], Loss: 4544.7407\n",
            "Epoch [200/1000], Loss: 3603.5732\n",
            "Epoch [200/1000], Loss: 2845.7432\n",
            "Epoch [200/1000], Loss: 4036.1323\n",
            "Epoch [200/1000], Loss: 3979.1506\n",
            "Epoch [200/1000], Loss: 4262.8154\n",
            "Epoch [200/1000], Loss: 4194.2295\n",
            "Epoch [210/1000], Loss: 3753.6606\n",
            "Epoch [210/1000], Loss: 3803.4075\n",
            "Epoch [210/1000], Loss: 3154.9072\n",
            "Epoch [210/1000], Loss: 2859.2961\n",
            "Epoch [210/1000], Loss: 4119.8267\n",
            "Epoch [210/1000], Loss: 4871.7397\n",
            "Epoch [210/1000], Loss: 4671.7886\n",
            "Epoch [210/1000], Loss: 1510.6752\n",
            "Epoch [220/1000], Loss: 3818.0149\n",
            "Epoch [220/1000], Loss: 4229.0815\n",
            "Epoch [220/1000], Loss: 3689.428\n",
            "Epoch [220/1000], Loss: 3820.9893\n",
            "Epoch [220/1000], Loss: 3477.7891\n",
            "Epoch [220/1000], Loss: 4028.0334\n",
            "Epoch [220/1000], Loss: 3784.1746\n",
            "Epoch [220/1000], Loss: 4475.1514\n",
            "Epoch [230/1000], Loss: 4324.377\n",
            "Epoch [230/1000], Loss: 3591.1899\n",
            "Epoch [230/1000], Loss: 3956.0256\n",
            "Epoch [230/1000], Loss: 3342.1621\n",
            "Epoch [230/1000], Loss: 3725.3149\n",
            "Epoch [230/1000], Loss: 3958.0137\n",
            "Epoch [230/1000], Loss: 3910.9011\n",
            "Epoch [230/1000], Loss: 1470.5502\n",
            "Epoch [240/1000], Loss: 3823.5249\n",
            "Epoch [240/1000], Loss: 3887.0264\n",
            "Epoch [240/1000], Loss: 3462.3086\n",
            "Epoch [240/1000], Loss: 3628.0852\n",
            "Epoch [240/1000], Loss: 4249.1431\n",
            "Epoch [240/1000], Loss: 3386.9336\n",
            "Epoch [240/1000], Loss: 4155.9854\n",
            "Epoch [240/1000], Loss: 1252.447\n",
            "Epoch [250/1000], Loss: 4338.2397\n",
            "Epoch [250/1000], Loss: 2736.8237\n",
            "Epoch [250/1000], Loss: 4151.126\n",
            "Epoch [250/1000], Loss: 3952.468\n",
            "Epoch [250/1000], Loss: 3284.0798\n",
            "Epoch [250/1000], Loss: 3335.2402\n",
            "Epoch [250/1000], Loss: 4397.6929\n",
            "Epoch [250/1000], Loss: 4450.5366\n",
            "Epoch [260/1000], Loss: 3361.5598\n",
            "Epoch [260/1000], Loss: 3631.3496\n",
            "Epoch [260/1000], Loss: 3866.947\n",
            "Epoch [260/1000], Loss: 4242.9766\n",
            "Epoch [260/1000], Loss: 4058.7388\n",
            "Epoch [260/1000], Loss: 3099.7969\n",
            "Epoch [260/1000], Loss: 3903.8755\n",
            "Epoch [260/1000], Loss: 2015.3666\n",
            "Epoch [270/1000], Loss: 4844.0068\n",
            "Epoch [270/1000], Loss: 2982.5269\n",
            "Epoch [270/1000], Loss: 3184.7236\n",
            "Epoch [270/1000], Loss: 4310.0122\n",
            "Epoch [270/1000], Loss: 3106.7949\n",
            "Epoch [270/1000], Loss: 4052.8943\n",
            "Epoch [270/1000], Loss: 3338.9331\n",
            "Epoch [270/1000], Loss: 4782.5137\n",
            "Epoch [280/1000], Loss: 3495.0967\n",
            "Epoch [280/1000], Loss: 4289.1851\n",
            "Epoch [280/1000], Loss: 3042.4739\n",
            "Epoch [280/1000], Loss: 3834.7881\n",
            "Epoch [280/1000], Loss: 3405.6936\n",
            "Epoch [280/1000], Loss: 3683.9165\n",
            "Epoch [280/1000], Loss: 4004.7849\n",
            "Epoch [280/1000], Loss: 2682.9221\n",
            "Epoch [290/1000], Loss: 3799.24\n",
            "Epoch [290/1000], Loss: 3209.5203\n",
            "Epoch [290/1000], Loss: 3873.6003\n",
            "Epoch [290/1000], Loss: 2829.3828\n",
            "Epoch [290/1000], Loss: 4653.6484\n",
            "Epoch [290/1000], Loss: 3783.2444\n",
            "Epoch [290/1000], Loss: 3321.4512\n",
            "Epoch [290/1000], Loss: 4453.543\n",
            "Epoch [300/1000], Loss: 3780.405\n",
            "Epoch [300/1000], Loss: 3918.5942\n",
            "Epoch [300/1000], Loss: 3124.4719\n",
            "Epoch [300/1000], Loss: 3651.0471\n",
            "Epoch [300/1000], Loss: 3851.5352\n",
            "Epoch [300/1000], Loss: 2985.9888\n",
            "Epoch [300/1000], Loss: 3966.8318\n",
            "Epoch [300/1000], Loss: 4588.4258\n",
            "Epoch [310/1000], Loss: 3443.5598\n",
            "Epoch [310/1000], Loss: 4068.884\n",
            "Epoch [310/1000], Loss: 2984.5977\n",
            "Epoch [310/1000], Loss: 4210.1655\n",
            "Epoch [310/1000], Loss: 3272.9993\n",
            "Epoch [310/1000], Loss: 3265.6775\n",
            "Epoch [310/1000], Loss: 4012.1543\n",
            "Epoch [310/1000], Loss: 1923.6304\n",
            "Epoch [320/1000], Loss: 4791.186\n",
            "Epoch [320/1000], Loss: 2900.03\n",
            "Epoch [320/1000], Loss: 2975.7131\n",
            "Epoch [320/1000], Loss: 3739.7349\n",
            "Epoch [320/1000], Loss: 3284.9189\n",
            "Epoch [320/1000], Loss: 4376.8589\n",
            "Epoch [320/1000], Loss: 3079.6218\n",
            "Epoch [320/1000], Loss: 1367.0121\n",
            "Epoch [330/1000], Loss: 3726.3582\n",
            "Epoch [330/1000], Loss: 3603.2512\n",
            "Epoch [330/1000], Loss: 3308.5613\n",
            "Epoch [330/1000], Loss: 4436.5054\n",
            "Epoch [330/1000], Loss: 3589.0037\n",
            "Epoch [330/1000], Loss: 3808.0554\n",
            "Epoch [330/1000], Loss: 2523.1619\n",
            "Epoch [330/1000], Loss: 1350.4032\n",
            "Epoch [340/1000], Loss: 2548.2253\n",
            "Epoch [340/1000], Loss: 3934.1912\n",
            "Epoch [340/1000], Loss: 4568.3091\n",
            "Epoch [340/1000], Loss: 3026.3604\n",
            "Epoch [340/1000], Loss: 3591.6472\n",
            "Epoch [340/1000], Loss: 3833.4209\n",
            "Epoch [340/1000], Loss: 3135.1277\n",
            "Epoch [340/1000], Loss: 4935.2354\n",
            "Epoch [350/1000], Loss: 4093.4622\n",
            "Epoch [350/1000], Loss: 3734.053\n",
            "Epoch [350/1000], Loss: 3952.9023\n",
            "Epoch [350/1000], Loss: 2656.6086\n",
            "Epoch [350/1000], Loss: 3390.7488\n",
            "Epoch [350/1000], Loss: 3326.2061\n",
            "Epoch [350/1000], Loss: 3540.0186\n",
            "Epoch [350/1000], Loss: 1564.9236\n",
            "Epoch [360/1000], Loss: 3780.72\n",
            "Epoch [360/1000], Loss: 3540.3875\n",
            "Epoch [360/1000], Loss: 3701.3708\n",
            "Epoch [360/1000], Loss: 3501.355\n",
            "Epoch [360/1000], Loss: 2410.6238\n",
            "Epoch [360/1000], Loss: 4443.9092\n",
            "Epoch [360/1000], Loss: 2942.3894\n",
            "Epoch [360/1000], Loss: 5297.5737\n",
            "Epoch [370/1000], Loss: 3674.2717\n",
            "Epoch [370/1000], Loss: 3461.1306\n",
            "Epoch [370/1000], Loss: 3211.1775\n",
            "Epoch [370/1000], Loss: 2910.8337\n",
            "Epoch [370/1000], Loss: 4181.0576\n",
            "Epoch [370/1000], Loss: 3833.7266\n",
            "Epoch [370/1000], Loss: 2942.9443\n",
            "Epoch [370/1000], Loss: 4705.5996\n",
            "Epoch [380/1000], Loss: 3897.5125\n",
            "Epoch [380/1000], Loss: 3883.2961\n",
            "Epoch [380/1000], Loss: 3485.6243\n",
            "Epoch [380/1000], Loss: 3677.7771\n",
            "Epoch [380/1000], Loss: 3025.5769\n",
            "Epoch [380/1000], Loss: 3384.5247\n",
            "Epoch [380/1000], Loss: 2796.2957\n",
            "Epoch [380/1000], Loss: 3679.5818\n",
            "Epoch [390/1000], Loss: 3851.5273\n",
            "Epoch [390/1000], Loss: 2889.1494\n",
            "Epoch [390/1000], Loss: 3389.6628\n",
            "Epoch [390/1000], Loss: 3514.1399\n",
            "Epoch [390/1000], Loss: 3469.0513\n",
            "Epoch [390/1000], Loss: 3231.0581\n",
            "Epoch [390/1000], Loss: 3745.2847\n",
            "Epoch [390/1000], Loss: 2342.1218\n",
            "Epoch [400/1000], Loss: 4057.2024\n",
            "Epoch [400/1000], Loss: 2507.7039\n",
            "Epoch [400/1000], Loss: 3415.3311\n",
            "Epoch [400/1000], Loss: 3666.6824\n",
            "Epoch [400/1000], Loss: 2746.5769\n",
            "Epoch [400/1000], Loss: 3906.3169\n",
            "Epoch [400/1000], Loss: 3356.2996\n",
            "Epoch [400/1000], Loss: 7379.3081\n",
            "Epoch [410/1000], Loss: 3117.5408\n",
            "Epoch [410/1000], Loss: 4069.1663\n",
            "Epoch [410/1000], Loss: 4064.5303\n",
            "Epoch [410/1000], Loss: 2628.9424\n",
            "Epoch [410/1000], Loss: 3907.6768\n",
            "Epoch [410/1000], Loss: 2779.1687\n",
            "Epoch [410/1000], Loss: 3036.2644\n",
            "Epoch [410/1000], Loss: 6246.9766\n",
            "Epoch [420/1000], Loss: 3453.3262\n",
            "Epoch [420/1000], Loss: 4376.6035\n",
            "Epoch [420/1000], Loss: 3006.0449\n",
            "Epoch [420/1000], Loss: 4322.1963\n",
            "Epoch [420/1000], Loss: 3325.3796\n",
            "Epoch [420/1000], Loss: 3038.4868\n",
            "Epoch [420/1000], Loss: 2214.0713\n",
            "Epoch [420/1000], Loss: 1804.7192\n",
            "Epoch [430/1000], Loss: 2936.595\n",
            "Epoch [430/1000], Loss: 3369.5559\n",
            "Epoch [430/1000], Loss: 3117.6567\n",
            "Epoch [430/1000], Loss: 3126.0686\n",
            "Epoch [430/1000], Loss: 4159.6592\n",
            "Epoch [430/1000], Loss: 3128.1555\n",
            "Epoch [430/1000], Loss: 3742.2148\n",
            "Epoch [430/1000], Loss: 2530.605\n",
            "Epoch [440/1000], Loss: 3199.8489\n",
            "Epoch [440/1000], Loss: 2463.2314\n",
            "Epoch [440/1000], Loss: 4298.2988\n",
            "Epoch [440/1000], Loss: 3022.3933\n",
            "Epoch [440/1000], Loss: 2630.8997\n",
            "Epoch [440/1000], Loss: 4167.3745\n",
            "Epoch [440/1000], Loss: 3709.478\n",
            "Epoch [440/1000], Loss: 1876.0454\n",
            "Epoch [450/1000], Loss: 2672.2461\n",
            "Epoch [450/1000], Loss: 3631.4912\n",
            "Epoch [450/1000], Loss: 3390.4587\n",
            "Epoch [450/1000], Loss: 3643.1675\n",
            "Epoch [450/1000], Loss: 3635.4961\n",
            "Epoch [450/1000], Loss: 3361.9612\n",
            "Epoch [450/1000], Loss: 2903.6343\n",
            "Epoch [450/1000], Loss: 4570.5488\n",
            "Epoch [460/1000], Loss: 3099.2786\n",
            "Epoch [460/1000], Loss: 3239.4626\n",
            "Epoch [460/1000], Loss: 4356.4966\n",
            "Epoch [460/1000], Loss: 3194.4587\n",
            "Epoch [460/1000], Loss: 3675.9043\n",
            "Epoch [460/1000], Loss: 2927.0017\n",
            "Epoch [460/1000], Loss: 2758.8325\n",
            "Epoch [460/1000], Loss: 2481.874\n",
            "Epoch [470/1000], Loss: 3607.1565\n",
            "Epoch [470/1000], Loss: 2468.8215\n",
            "Epoch [470/1000], Loss: 2286.1914\n",
            "Epoch [470/1000], Loss: 4313.9712\n",
            "Epoch [470/1000], Loss: 3043.6692\n",
            "Epoch [470/1000], Loss: 3695.7112\n",
            "Epoch [470/1000], Loss: 3713.2432\n",
            "Epoch [470/1000], Loss: 2764.7627\n",
            "Epoch [480/1000], Loss: 3541.0125\n",
            "Epoch [480/1000], Loss: 2922.9331\n",
            "Epoch [480/1000], Loss: 2914.8518\n",
            "Epoch [480/1000], Loss: 2850.4712\n",
            "Epoch [480/1000], Loss: 2561.5234\n",
            "Epoch [480/1000], Loss: 4365.665\n",
            "Epoch [480/1000], Loss: 3982.7629\n",
            "Epoch [480/1000], Loss: 1022.9257\n",
            "Epoch [490/1000], Loss: 3243.0972\n",
            "Epoch [490/1000], Loss: 4106.9243\n",
            "Epoch [490/1000], Loss: 2763.3372\n",
            "Epoch [490/1000], Loss: 3845.198\n",
            "Epoch [490/1000], Loss: 3070.4524\n",
            "Epoch [490/1000], Loss: 2986.6174\n",
            "Epoch [490/1000], Loss: 3032.5413\n",
            "Epoch [490/1000], Loss: 640.2617\n",
            "Epoch [500/1000], Loss: 3318.4255\n",
            "Epoch [500/1000], Loss: 3617.6353\n",
            "Epoch [500/1000], Loss: 3517.9768\n",
            "Epoch [500/1000], Loss: 3039.2454\n",
            "Epoch [500/1000], Loss: 3312.9526\n",
            "Epoch [500/1000], Loss: 3022.584\n",
            "Epoch [500/1000], Loss: 3026.54\n",
            "Epoch [500/1000], Loss: 2234.8691\n",
            "Epoch [510/1000], Loss: 2934.3193\n",
            "Epoch [510/1000], Loss: 3153.8687\n",
            "Epoch [510/1000], Loss: 4054.2053\n",
            "Epoch [510/1000], Loss: 3076.8899\n",
            "Epoch [510/1000], Loss: 3149.9329\n",
            "Epoch [510/1000], Loss: 2434.085\n",
            "Epoch [510/1000], Loss: 4108.4355\n",
            "Epoch [510/1000], Loss: 30.7092\n",
            "Epoch [520/1000], Loss: 2670.3689\n",
            "Epoch [520/1000], Loss: 3004.7766\n",
            "Epoch [520/1000], Loss: 2610.6133\n",
            "Epoch [520/1000], Loss: 4314.2925\n",
            "Epoch [520/1000], Loss: 4304.4639\n",
            "Epoch [520/1000], Loss: 2325.9934\n",
            "Epoch [520/1000], Loss: 3260.4419\n",
            "Epoch [520/1000], Loss: 5556.9097\n",
            "Epoch [530/1000], Loss: 2769.001\n",
            "Epoch [530/1000], Loss: 3313.5417\n",
            "Epoch [530/1000], Loss: 3769.3679\n",
            "Epoch [530/1000], Loss: 3170.3811\n",
            "Epoch [530/1000], Loss: 2199.7927\n",
            "Epoch [530/1000], Loss: 3625.6321\n",
            "Epoch [530/1000], Loss: 3844.813\n",
            "Epoch [530/1000], Loss: 799.5788\n",
            "Epoch [540/1000], Loss: 4096.5269\n",
            "Epoch [540/1000], Loss: 3176.105\n",
            "Epoch [540/1000], Loss: 3385.4089\n",
            "Epoch [540/1000], Loss: 3269.7271\n",
            "Epoch [540/1000], Loss: 2561.8921\n",
            "Epoch [540/1000], Loss: 3558.9277\n",
            "Epoch [540/1000], Loss: 2546.2517\n",
            "Epoch [540/1000], Loss: 968.5315\n",
            "Epoch [550/1000], Loss: 3801.1665\n",
            "Epoch [550/1000], Loss: 3649.4119\n",
            "Epoch [550/1000], Loss: 3399.4421\n",
            "Epoch [550/1000], Loss: 2324.4858\n",
            "Epoch [550/1000], Loss: 3367.9761\n",
            "Epoch [550/1000], Loss: 2687.1187\n",
            "Epoch [550/1000], Loss: 3197.5886\n",
            "Epoch [550/1000], Loss: 2542.6577\n",
            "Epoch [560/1000], Loss: 3007.9648\n",
            "Epoch [560/1000], Loss: 3391.9758\n",
            "Epoch [560/1000], Loss: 3336.354\n",
            "Epoch [560/1000], Loss: 2816.0105\n",
            "Epoch [560/1000], Loss: 3402.0762\n",
            "Epoch [560/1000], Loss: 3004.3743\n",
            "Epoch [560/1000], Loss: 3323.5989\n",
            "Epoch [560/1000], Loss: 3679.5356\n",
            "Epoch [570/1000], Loss: 2200.0295\n",
            "Epoch [570/1000], Loss: 3107.8606\n",
            "Epoch [570/1000], Loss: 3744.8923\n",
            "Epoch [570/1000], Loss: 4922.9087\n",
            "Epoch [570/1000], Loss: 2510.8047\n",
            "Epoch [570/1000], Loss: 2734.0522\n",
            "Epoch [570/1000], Loss: 3053.4792\n",
            "Epoch [570/1000], Loss: 2673.3677\n",
            "Epoch [580/1000], Loss: 2423.6309\n",
            "Epoch [580/1000], Loss: 2716.1636\n",
            "Epoch [580/1000], Loss: 3122.7224\n",
            "Epoch [580/1000], Loss: 2661.5891\n",
            "Epoch [580/1000], Loss: 3192.3074\n",
            "Epoch [580/1000], Loss: 3916.9617\n",
            "Epoch [580/1000], Loss: 4258.5933\n",
            "Epoch [580/1000], Loss: 1749.6589\n",
            "Epoch [590/1000], Loss: 2849.8606\n",
            "Epoch [590/1000], Loss: 3655.3184\n",
            "Epoch [590/1000], Loss: 3153.9016\n",
            "Epoch [590/1000], Loss: 3268.4355\n",
            "Epoch [590/1000], Loss: 3438.0217\n",
            "Epoch [590/1000], Loss: 2395.1128\n",
            "Epoch [590/1000], Loss: 3322.2996\n",
            "Epoch [590/1000], Loss: 3673.3953\n",
            "Epoch [600/1000], Loss: 2485.698\n",
            "Epoch [600/1000], Loss: 3701.8765\n",
            "Epoch [600/1000], Loss: 3440.937\n",
            "Epoch [600/1000], Loss: 3497.438\n",
            "Epoch [600/1000], Loss: 2971.6475\n",
            "Epoch [600/1000], Loss: 3011.3975\n",
            "Epoch [600/1000], Loss: 3094.4509\n",
            "Epoch [600/1000], Loss: 645.8928\n",
            "Epoch [610/1000], Loss: 2611.3921\n",
            "Epoch [610/1000], Loss: 2965.7009\n",
            "Epoch [610/1000], Loss: 3585.3594\n",
            "Epoch [610/1000], Loss: 2604.0977\n",
            "Epoch [610/1000], Loss: 2708.3931\n",
            "Epoch [610/1000], Loss: 3202.6719\n",
            "Epoch [610/1000], Loss: 4329.6182\n",
            "Epoch [610/1000], Loss: 2851.5056\n",
            "Epoch [620/1000], Loss: 2794.0249\n",
            "Epoch [620/1000], Loss: 3882.0337\n",
            "Epoch [620/1000], Loss: 2550.5603\n",
            "Epoch [620/1000], Loss: 3256.3889\n",
            "Epoch [620/1000], Loss: 3970.2373\n",
            "Epoch [620/1000], Loss: 2446.6941\n",
            "Epoch [620/1000], Loss: 3142.5684\n",
            "Epoch [620/1000], Loss: 1336.6643\n",
            "Epoch [630/1000], Loss: 2906.9924\n",
            "Epoch [630/1000], Loss: 3260.4761\n",
            "Epoch [630/1000], Loss: 2904.1956\n",
            "Epoch [630/1000], Loss: 3723.5249\n",
            "Epoch [630/1000], Loss: 2867.7224\n",
            "Epoch [630/1000], Loss: 3128.978\n",
            "Epoch [630/1000], Loss: 2885.77\n",
            "Epoch [630/1000], Loss: 6543.2319\n",
            "Epoch [640/1000], Loss: 3061.2573\n",
            "Epoch [640/1000], Loss: 2888.4556\n",
            "Epoch [640/1000], Loss: 2797.7781\n",
            "Epoch [640/1000], Loss: 2205.3306\n",
            "Epoch [640/1000], Loss: 4137.0522\n",
            "Epoch [640/1000], Loss: 3087.3052\n",
            "Epoch [640/1000], Loss: 3374.2986\n",
            "Epoch [640/1000], Loss: 7235.9365\n",
            "Epoch [650/1000], Loss: 2807.7849\n",
            "Epoch [650/1000], Loss: 2929.4299\n",
            "Epoch [650/1000], Loss: 2490.3376\n",
            "Epoch [650/1000], Loss: 3822.8247\n",
            "Epoch [650/1000], Loss: 3102.085\n",
            "Epoch [650/1000], Loss: 3202.2236\n",
            "Epoch [650/1000], Loss: 3511.1868\n",
            "Epoch [650/1000], Loss: 946.3981\n",
            "Epoch [660/1000], Loss: 3405.9561\n",
            "Epoch [660/1000], Loss: 3410.1174\n",
            "Epoch [660/1000], Loss: 2482.8984\n",
            "Epoch [660/1000], Loss: 2807.0916\n",
            "Epoch [660/1000], Loss: 2792.6462\n",
            "Epoch [660/1000], Loss: 2953.8164\n",
            "Epoch [660/1000], Loss: 3769.6948\n",
            "Epoch [660/1000], Loss: 4157.5444\n",
            "Epoch [670/1000], Loss: 3722.5442\n",
            "Epoch [670/1000], Loss: 3276.136\n",
            "Epoch [670/1000], Loss: 1932.4565\n",
            "Epoch [670/1000], Loss: 3558.6575\n",
            "Epoch [670/1000], Loss: 3587.8794\n",
            "Epoch [670/1000], Loss: 2757.1731\n",
            "Epoch [670/1000], Loss: 2871.843\n",
            "Epoch [670/1000], Loss: 1792.469\n",
            "Epoch [680/1000], Loss: 3871.8577\n",
            "Epoch [680/1000], Loss: 3171.6589\n",
            "Epoch [680/1000], Loss: 2565.1487\n",
            "Epoch [680/1000], Loss: 2860.9658\n",
            "Epoch [680/1000], Loss: 2819.7656\n",
            "Epoch [680/1000], Loss: 4023.1287\n",
            "Epoch [680/1000], Loss: 2388.738\n",
            "Epoch [680/1000], Loss: 1099.8807\n",
            "Epoch [690/1000], Loss: 2280.9309\n",
            "Epoch [690/1000], Loss: 2801.2644\n",
            "Epoch [690/1000], Loss: 2929.1709\n",
            "Epoch [690/1000], Loss: 3042.8708\n",
            "Epoch [690/1000], Loss: 3499.3142\n",
            "Epoch [690/1000], Loss: 3235.2849\n",
            "Epoch [690/1000], Loss: 3629.3228\n",
            "Epoch [690/1000], Loss: 4928.1992\n",
            "Epoch [700/1000], Loss: 3329.8286\n",
            "Epoch [700/1000], Loss: 2587.3831\n",
            "Epoch [700/1000], Loss: 3285.6941\n",
            "Epoch [700/1000], Loss: 2977.0999\n",
            "Epoch [700/1000], Loss: 3112.8994\n",
            "Epoch [700/1000], Loss: 3071.9001\n",
            "Epoch [700/1000], Loss: 3246.9692\n",
            "Epoch [700/1000], Loss: 935.3757\n",
            "Epoch [710/1000], Loss: 2479.6345\n",
            "Epoch [710/1000], Loss: 3552.5215\n",
            "Epoch [710/1000], Loss: 3093.6174\n",
            "Epoch [710/1000], Loss: 3839.9727\n",
            "Epoch [710/1000], Loss: 3181.3979\n",
            "Epoch [710/1000], Loss: 2868.9915\n",
            "Epoch [710/1000], Loss: 2586.7832\n",
            "Epoch [710/1000], Loss: 567.6285\n",
            "Epoch [720/1000], Loss: 2676.5234\n",
            "Epoch [720/1000], Loss: 3857.8699\n",
            "Epoch [720/1000], Loss: 3749.21\n",
            "Epoch [720/1000], Loss: 2724.498\n",
            "Epoch [720/1000], Loss: 2529.105\n",
            "Epoch [720/1000], Loss: 2638.303\n",
            "Epoch [720/1000], Loss: 3319.5862\n",
            "Epoch [720/1000], Loss: 1401.3828\n",
            "Epoch [730/1000], Loss: 3530.6599\n",
            "Epoch [730/1000], Loss: 3297.9712\n",
            "Epoch [730/1000], Loss: 2779.8511\n",
            "Epoch [730/1000], Loss: 3180.0403\n",
            "Epoch [730/1000], Loss: 3260.2437\n",
            "Epoch [730/1000], Loss: 2399.2844\n",
            "Epoch [730/1000], Loss: 2874.228\n",
            "Epoch [730/1000], Loss: 3530.2661\n",
            "Epoch [740/1000], Loss: 3553.1221\n",
            "Epoch [740/1000], Loss: 3121.3972\n",
            "Epoch [740/1000], Loss: 2479.2383\n",
            "Epoch [740/1000], Loss: 3177.3105\n",
            "Epoch [740/1000], Loss: 2779.8181\n",
            "Epoch [740/1000], Loss: 3196.3411\n",
            "Epoch [740/1000], Loss: 3122.6667\n",
            "Epoch [740/1000], Loss: 1077.8722\n",
            "Epoch [750/1000], Loss: 1788.4265\n",
            "Epoch [750/1000], Loss: 3666.8755\n",
            "Epoch [750/1000], Loss: 3278.1519\n",
            "Epoch [750/1000], Loss: 2906.6072\n",
            "Epoch [750/1000], Loss: 3454.0537\n",
            "Epoch [750/1000], Loss: 3290.8008\n",
            "Epoch [750/1000], Loss: 2958.3582\n",
            "Epoch [750/1000], Loss: 1964.6639\n",
            "Epoch [760/1000], Loss: 3603.6423\n",
            "Epoch [760/1000], Loss: 2906.6565\n",
            "Epoch [760/1000], Loss: 2729.2476\n",
            "Epoch [760/1000], Loss: 2548.4731\n",
            "Epoch [760/1000], Loss: 2180.814\n",
            "Epoch [760/1000], Loss: 3590.0442\n",
            "Epoch [760/1000], Loss: 3730.3005\n",
            "Epoch [760/1000], Loss: 2527.6567\n",
            "Epoch [770/1000], Loss: 2918.5117\n",
            "Epoch [770/1000], Loss: 3451.5183\n",
            "Epoch [770/1000], Loss: 2974.8767\n",
            "Epoch [770/1000], Loss: 3603.5693\n",
            "Epoch [770/1000], Loss: 2557.5713\n",
            "Epoch [770/1000], Loss: 2698.8303\n",
            "Epoch [770/1000], Loss: 3143.6562\n",
            "Epoch [770/1000], Loss: 765.0735\n",
            "Epoch [780/1000], Loss: 4133.8521\n",
            "Epoch [780/1000], Loss: 2495.1831\n",
            "Epoch [780/1000], Loss: 2283.373\n",
            "Epoch [780/1000], Loss: 2211.3711\n",
            "Epoch [780/1000], Loss: 3775.5498\n",
            "Epoch [780/1000], Loss: 3675.189\n",
            "Epoch [780/1000], Loss: 2594.7878\n",
            "Epoch [780/1000], Loss: 2941.2598\n",
            "Epoch [790/1000], Loss: 2860.7781\n",
            "Epoch [790/1000], Loss: 3500.0337\n",
            "Epoch [790/1000], Loss: 2662.1943\n",
            "Epoch [790/1000], Loss: 2837.1855\n",
            "Epoch [790/1000], Loss: 3187.3887\n",
            "Epoch [790/1000], Loss: 2600.9036\n",
            "Epoch [790/1000], Loss: 3645.7122\n",
            "Epoch [790/1000], Loss: 392.2766\n",
            "Epoch [800/1000], Loss: 3100.1919\n",
            "Epoch [800/1000], Loss: 2522.3298\n",
            "Epoch [800/1000], Loss: 2888.0039\n",
            "Epoch [800/1000], Loss: 2902.3354\n",
            "Epoch [800/1000], Loss: 3731.1067\n",
            "Epoch [800/1000], Loss: 2172.074\n",
            "Epoch [800/1000], Loss: 3446.8767\n",
            "Epoch [800/1000], Loss: 8792.1357\n",
            "Epoch [810/1000], Loss: 3138.675\n",
            "Epoch [810/1000], Loss: 2926.239\n",
            "Epoch [810/1000], Loss: 2895.6555\n",
            "Epoch [810/1000], Loss: 2219.5171\n",
            "Epoch [810/1000], Loss: 3293.668\n",
            "Epoch [810/1000], Loss: 3451.8394\n",
            "Epoch [810/1000], Loss: 3227.3198\n",
            "Epoch [810/1000], Loss: 1669.2363\n",
            "Epoch [820/1000], Loss: 4034.7747\n",
            "Epoch [820/1000], Loss: 2809.136\n",
            "Epoch [820/1000], Loss: 4142.6172\n",
            "Epoch [820/1000], Loss: 3431.8455\n",
            "Epoch [820/1000], Loss: 2638.4927\n",
            "Epoch [820/1000], Loss: 2124.418\n",
            "Epoch [820/1000], Loss: 2040.5028\n",
            "Epoch [820/1000], Loss: 238.5059\n",
            "Epoch [830/1000], Loss: 3395.5764\n",
            "Epoch [830/1000], Loss: 3233.2517\n",
            "Epoch [830/1000], Loss: 3183.7527\n",
            "Epoch [830/1000], Loss: 3278.2666\n",
            "Epoch [830/1000], Loss: 3090.855\n",
            "Epoch [830/1000], Loss: 2318.3193\n",
            "Epoch [830/1000], Loss: 2632.8901\n",
            "Epoch [830/1000], Loss: 1144.7028\n",
            "Epoch [840/1000], Loss: 2785.9614\n",
            "Epoch [840/1000], Loss: 3480.4988\n",
            "Epoch [840/1000], Loss: 2871.8403\n",
            "Epoch [840/1000], Loss: 2441.9543\n",
            "Epoch [840/1000], Loss: 2645.854\n",
            "Epoch [840/1000], Loss: 2754.0513\n",
            "Epoch [840/1000], Loss: 3956.8542\n",
            "Epoch [840/1000], Loss: 3946.106\n",
            "Epoch [850/1000], Loss: 3237.5273\n",
            "Epoch [850/1000], Loss: 2741.0188\n",
            "Epoch [850/1000], Loss: 2846.3633\n",
            "Epoch [850/1000], Loss: 3065.6284\n",
            "Epoch [850/1000], Loss: 2742.9285\n",
            "Epoch [850/1000], Loss: 3109.605\n",
            "Epoch [850/1000], Loss: 3183.8149\n",
            "Epoch [850/1000], Loss: 3587.3503\n",
            "Epoch [860/1000], Loss: 2629.8284\n",
            "Epoch [860/1000], Loss: 3514.8113\n",
            "Epoch [860/1000], Loss: 3369.3398\n",
            "Epoch [860/1000], Loss: 2720.9958\n",
            "Epoch [860/1000], Loss: 2391.3503\n",
            "Epoch [860/1000], Loss: 3166.8794\n",
            "Epoch [860/1000], Loss: 3315.9878\n",
            "Epoch [860/1000], Loss: 433.4401\n",
            "Epoch [870/1000], Loss: 2974.2478\n",
            "Epoch [870/1000], Loss: 4309.3018\n",
            "Epoch [870/1000], Loss: 3097.915\n",
            "Epoch [870/1000], Loss: 3076.178\n",
            "Epoch [870/1000], Loss: 3687.2734\n",
            "Epoch [870/1000], Loss: 1992.5577\n",
            "Epoch [870/1000], Loss: 1829.316\n",
            "Epoch [870/1000], Loss: 1879.4141\n",
            "Epoch [880/1000], Loss: 3139.5222\n",
            "Epoch [880/1000], Loss: 3304.7136\n",
            "Epoch [880/1000], Loss: 3674.4353\n",
            "Epoch [880/1000], Loss: 2843.9414\n",
            "Epoch [880/1000], Loss: 2641.8899\n",
            "Epoch [880/1000], Loss: 2602.2322\n",
            "Epoch [880/1000], Loss: 2745.4136\n",
            "Epoch [880/1000], Loss: 1739.426\n",
            "Epoch [890/1000], Loss: 3998.1843\n",
            "Epoch [890/1000], Loss: 2209.2991\n",
            "Epoch [890/1000], Loss: 3021.6594\n",
            "Epoch [890/1000], Loss: 2768.3481\n",
            "Epoch [890/1000], Loss: 2598.2461\n",
            "Epoch [890/1000], Loss: 3657.6411\n",
            "Epoch [890/1000], Loss: 2734.9856\n",
            "Epoch [890/1000], Loss: 824.2297\n",
            "Epoch [900/1000], Loss: 3287.21\n",
            "Epoch [900/1000], Loss: 2857.9583\n",
            "Epoch [900/1000], Loss: 3107.1743\n",
            "Epoch [900/1000], Loss: 3465.9783\n",
            "Epoch [900/1000], Loss: 2722.5427\n",
            "Epoch [900/1000], Loss: 3126.324\n",
            "Epoch [900/1000], Loss: 2318.4075\n",
            "Epoch [900/1000], Loss: 2096.1184\n",
            "Epoch [910/1000], Loss: 2452.9163\n",
            "Epoch [910/1000], Loss: 2559.2656\n",
            "Epoch [910/1000], Loss: 3741.6379\n",
            "Epoch [910/1000], Loss: 2865.6082\n",
            "Epoch [910/1000], Loss: 2505.8967\n",
            "Epoch [910/1000], Loss: 2913.5056\n",
            "Epoch [910/1000], Loss: 3828.0168\n",
            "Epoch [910/1000], Loss: 2285.3113\n",
            "Epoch [920/1000], Loss: 2848.0293\n",
            "Epoch [920/1000], Loss: 2862.5051\n",
            "Epoch [920/1000], Loss: 3417.6184\n",
            "Epoch [920/1000], Loss: 3022.7871\n",
            "Epoch [920/1000], Loss: 2786.8962\n",
            "Epoch [920/1000], Loss: 3075.1572\n",
            "Epoch [920/1000], Loss: 2867.8142\n",
            "Epoch [920/1000], Loss: 1546.6016\n",
            "Epoch [930/1000], Loss: 2802.3018\n",
            "Epoch [930/1000], Loss: 3192.9993\n",
            "Epoch [930/1000], Loss: 2426.5547\n",
            "Epoch [930/1000], Loss: 2801.1753\n",
            "Epoch [930/1000], Loss: 2716.1572\n",
            "Epoch [930/1000], Loss: 3379.6416\n",
            "Epoch [930/1000], Loss: 3468.3286\n",
            "Epoch [930/1000], Loss: 2698.0476\n",
            "Epoch [940/1000], Loss: 2723.1138\n",
            "Epoch [940/1000], Loss: 3066.3381\n",
            "Epoch [940/1000], Loss: 3274.074\n",
            "Epoch [940/1000], Loss: 3886.8276\n",
            "Epoch [940/1000], Loss: 2277.7222\n",
            "Epoch [940/1000], Loss: 3100.3018\n",
            "Epoch [940/1000], Loss: 2462.4858\n",
            "Epoch [940/1000], Loss: 2467.7468\n",
            "Epoch [950/1000], Loss: 2834.3708\n",
            "Epoch [950/1000], Loss: 2543.0342\n",
            "Epoch [950/1000], Loss: 3221.418\n",
            "Epoch [950/1000], Loss: 2463.4631\n",
            "Epoch [950/1000], Loss: 2641.2761\n",
            "Epoch [950/1000], Loss: 2952.0989\n",
            "Epoch [950/1000], Loss: 3983.0093\n",
            "Epoch [950/1000], Loss: 4454.9756\n",
            "Epoch [960/1000], Loss: 2564.3921\n",
            "Epoch [960/1000], Loss: 2018.2799\n",
            "Epoch [960/1000], Loss: 3057.8313\n",
            "Epoch [960/1000], Loss: 2893.5117\n",
            "Epoch [960/1000], Loss: 3868.3599\n",
            "Epoch [960/1000], Loss: 3637.269\n",
            "Epoch [960/1000], Loss: 2692.5669\n",
            "Epoch [960/1000], Loss: 2508.4397\n",
            "Epoch [970/1000], Loss: 3761.7917\n",
            "Epoch [970/1000], Loss: 3088.7048\n",
            "Epoch [970/1000], Loss: 2341.2258\n",
            "Epoch [970/1000], Loss: 2681.0281\n",
            "Epoch [970/1000], Loss: 2480.8301\n",
            "Epoch [970/1000], Loss: 3531.2258\n",
            "Epoch [970/1000], Loss: 2959.1353\n",
            "Epoch [970/1000], Loss: 139.0307\n",
            "Epoch [980/1000], Loss: 2504.8459\n",
            "Epoch [980/1000], Loss: 2989.1924\n",
            "Epoch [980/1000], Loss: 3262.7927\n",
            "Epoch [980/1000], Loss: 3279.5881\n",
            "Epoch [980/1000], Loss: 3314.7617\n",
            "Epoch [980/1000], Loss: 2870.53\n",
            "Epoch [980/1000], Loss: 2566.8274\n",
            "Epoch [980/1000], Loss: 857.9698\n",
            "Epoch [990/1000], Loss: 2792.5625\n",
            "Epoch [990/1000], Loss: 2922.1541\n",
            "Epoch [990/1000], Loss: 2418.0999\n",
            "Epoch [990/1000], Loss: 2588.5864\n",
            "Epoch [990/1000], Loss: 3784.5613\n",
            "Epoch [990/1000], Loss: 3490.5681\n",
            "Epoch [990/1000], Loss: 2701.1174\n",
            "Epoch [990/1000], Loss: 2218.3738\n",
            "Epoch [1000/1000], Loss: 2272.6326\n",
            "Epoch [1000/1000], Loss: 2809.5527\n",
            "Epoch [1000/1000], Loss: 2537.9714\n",
            "Epoch [1000/1000], Loss: 2520.5352\n",
            "Epoch [1000/1000], Loss: 3242.0283\n",
            "Epoch [1000/1000], Loss: 3624.4241\n",
            "Epoch [1000/1000], Loss: 3516.6943\n",
            "Epoch [1000/1000], Loss: 4823.5469\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can see loss is significantly lower at the end than it was at the start. However, it is also bouncing around a little still which suggests the model needs more training (100 epochs is not a lot in deep learning terms). However, let's evaluate as before:"
      ],
      "metadata": {
        "id": "E72ZTKSqAODE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Evaluation (example)\n",
        "model.eval() # testing mode\n",
        "mse_values = [] # collect the MSE scores\n",
        "\n",
        "with torch.no_grad():\n",
        "    for inputs, targets in test_loader:\n",
        "        inputs, targets = inputs.to(device), targets.to(device)\n",
        "        outputs = model(inputs) # predict the test data\n",
        "\n",
        "        # Calculate Mean Squared Error\n",
        "        mse = criterion(outputs, targets) # calcualte mse for the batch\n",
        "        mse_values.append(mse.item()) # add to the list of MSE values\n",
        "\n",
        "# Calculate and print the average MSE\n",
        "avg_mse = np.mean(mse_values)\n",
        "print(f\"Average MSE on test set: {avg_mse}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RbuAH6p8A-Vh",
        "outputId": "a84eec19-5d20-4485-d246-22a943024fa1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Average MSE on test set: 2855.1199951171875\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "MSE looks expected given training (no obvious sign of overfitting). However, we probably can get better results with tuning and more epochs.\n",
        "\n",
        "Let's run the loop again a little differently to collect the predicted values (y_hat) and actuals (y) and add them to a dataset for comparions:"
      ],
      "metadata": {
        "id": "HQ26bA08Up12"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Evaluation\n",
        "model.eval()\n",
        "predictions = []\n",
        "actuals = []\n",
        "\n",
        "with torch.no_grad():\n",
        "    for inputs, targets in test_loader:\n",
        "        inputs, targets = inputs.to(device), targets.to(device)\n",
        "        outputs = model(inputs)\n",
        "        predictions.extend(outputs.cpu().numpy())\n",
        "        actuals.extend(targets.cpu().numpy())\n",
        "\n",
        "# Create DataFrame\n",
        "results_df = pd.DataFrame({'Predicted': np.array(predictions).flatten(), 'Actual': np.array(actuals).flatten()})\n",
        "results_df"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 419
        },
        "id": "8AYsDDSLUp_u",
        "outputId": "89d5a56d-2b61-4712-c69f-bf3215e85007"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "     Predicted  Actual\n",
              "0   146.128174   219.0\n",
              "1   175.910110    70.0\n",
              "2   142.108093   202.0\n",
              "3   294.398193   230.0\n",
              "4   128.379059   111.0\n",
              "..         ...     ...\n",
              "84  116.957664   153.0\n",
              "85   89.024422    98.0\n",
              "86   77.176537    37.0\n",
              "87   66.588280    63.0\n",
              "88  153.455658   184.0\n",
              "\n",
              "[89 rows x 2 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-7e7e7e0a-9809-497c-8be6-37a0c4028fb8\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Predicted</th>\n",
              "      <th>Actual</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>146.128174</td>\n",
              "      <td>219.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>175.910110</td>\n",
              "      <td>70.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>142.108093</td>\n",
              "      <td>202.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>294.398193</td>\n",
              "      <td>230.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>128.379059</td>\n",
              "      <td>111.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>84</th>\n",
              "      <td>116.957664</td>\n",
              "      <td>153.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>85</th>\n",
              "      <td>89.024422</td>\n",
              "      <td>98.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>86</th>\n",
              "      <td>77.176537</td>\n",
              "      <td>37.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>87</th>\n",
              "      <td>66.588280</td>\n",
              "      <td>63.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>88</th>\n",
              "      <td>153.455658</td>\n",
              "      <td>184.0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>89 rows  2 columns</p>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-7e7e7e0a-9809-497c-8be6-37a0c4028fb8')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-7e7e7e0a-9809-497c-8be6-37a0c4028fb8 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-7e7e7e0a-9809-497c-8be6-37a0c4028fb8');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "  <div id=\"id_33d308b8-4356-4d27-90d8-a9313f77e494\">\n",
              "    <style>\n",
              "      .colab-df-generate {\n",
              "        background-color: #E8F0FE;\n",
              "        border: none;\n",
              "        border-radius: 50%;\n",
              "        cursor: pointer;\n",
              "        display: none;\n",
              "        fill: #1967D2;\n",
              "        height: 32px;\n",
              "        padding: 0 0 0 0;\n",
              "        width: 32px;\n",
              "      }\n",
              "\n",
              "      .colab-df-generate:hover {\n",
              "        background-color: #E2EBFA;\n",
              "        box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "        fill: #174EA6;\n",
              "      }\n",
              "\n",
              "      [theme=dark] .colab-df-generate {\n",
              "        background-color: #3B4455;\n",
              "        fill: #D2E3FC;\n",
              "      }\n",
              "\n",
              "      [theme=dark] .colab-df-generate:hover {\n",
              "        background-color: #434B5C;\n",
              "        box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "        filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "        fill: #FFFFFF;\n",
              "      }\n",
              "    </style>\n",
              "    <button class=\"colab-df-generate\" onclick=\"generateWithVariable('results_df')\"\n",
              "            title=\"Generate code using this dataframe.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M7,19H8.4L18.45,9,17,7.55,7,17.6ZM5,21V16.75L18.45,3.32a2,2,0,0,1,2.83,0l1.4,1.43a1.91,1.91,0,0,1,.58,1.4,1.91,1.91,0,0,1-.58,1.4L9.25,21ZM18.45,9,17,7.55Zm-12,3A5.31,5.31,0,0,0,4.9,8.1,5.31,5.31,0,0,0,1,6.5,5.31,5.31,0,0,0,4.9,4.9,5.31,5.31,0,0,0,6.5,1,5.31,5.31,0,0,0,8.1,4.9,5.31,5.31,0,0,0,12,6.5,5.46,5.46,0,0,0,6.5,12Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "    <script>\n",
              "      (() => {\n",
              "      const buttonEl =\n",
              "        document.querySelector('#id_33d308b8-4356-4d27-90d8-a9313f77e494 button.colab-df-generate');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      buttonEl.onclick = () => {\n",
              "        google.colab.notebook.generateWithVariable('results_df');\n",
              "      }\n",
              "      })();\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "variable_name": "results_df",
              "summary": "{\n  \"name\": \"results_df\",\n  \"rows\": 89,\n  \"fields\": [\n    {\n      \"column\": \"Predicted\",\n      \"properties\": {\n        \"dtype\": \"float32\",\n        \"num_unique_values\": 89,\n        \"samples\": [\n          176.9022674560547,\n          112.09540557861328,\n          177.4256591796875\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Actual\",\n      \"properties\": {\n        \"dtype\": \"float32\",\n        \"num_unique_values\": 75,\n        \"samples\": [\n          111.0,\n          61.0,\n          252.0\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"
            }
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Side-by-side, they don't look great. Can you improve them?\n",
        "\n",
        "<br><br>\n",
        "\n",
        "## EXERCISE #1\n",
        "Try increasing the number of epochs to 1,000 (when the model is fairly well trained then the results printed for each 10x epochs will be fairly stable and not change much). Does this give better results?\n",
        "\n",
        "<br><br>\n",
        "\n",
        "## EXERCISE #2 (optional)\n",
        "Try experimenting with the architecture (number of neurons and/or number of layers). Can we reach an optimal architecture?"
      ],
      "metadata": {
        "id": "LDcM98lHbgP8"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "424831a5"
      },
      "source": [
        "### Results after Training with 1000 Epochs\n",
        "\n",
        "After increasing the number of training epochs to 1000, the average Mean Squared Error (MSE) on the test set was approximately `2855.12`. This is comparable to the MSE obtained with 100 epochs, suggesting that simply training for more epochs did not lead to a significant improvement in model performance for this dataset. The model may have converged earlier, or further improvements might require architectural changes or hyperparameter tuning.\n",
        "\n",
        "The side-by-side comparison of predicted versus actual values in the `results_df` indicates that while the model captures some general trends, there is still a noticeable discrepancy between the predictions and the true values. This reinforces the idea that further optimization beyond just increasing epochs is likely needed to achieve better predictive accuracy."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "afb85568"
      },
      "source": [
        "## Summary of Data Science and Modeling Process\n",
        "\n",
        "This notebook demonstrates a basic end-to-end process for building and evaluating a Dense Neural Network for a regression problem using PyTorch, specifically predicting diabetes progression.\n",
        "\n",
        "### 1. Data Loading and Understanding\n",
        "*   **Dataset:** We used the in-built `load_diabetes()` dataset from `sklearn.datasets`. This dataset consists of **10 baseline variables** (age, sex, body mass index, average blood pressure, and six blood serum measurements) as features, and a quantitative measure of disease progression as the target.\n",
        "*   **Nature of Data:** The data is 'structured' and already cleaned and normalized, simplifying the preprocessing steps.\n",
        "\n",
        "### 2. Data Preparation for PyTorch\n",
        "*   **Tensor Conversion:** The `numpy` arrays from `sklearn` were converted into PyTorch tensors using `torch.tensor()`.\n",
        "*   **Target Reshaping:** The target variable `y` was reshaped to `(-1, 1)` to ensure it's a column vector, which is often required for PyTorch's loss functions.\n",
        "*   **Train/Test Split:** The dataset was split into training (80%) and testing (20%) sets using `train_test_split` from `sklearn.model_selection`.\n",
        "*   **DataLoaders:** `TensorDataset` and `DataLoader` from `torch.utils.data` were used to create iterable batches of data, enabling efficient training.\n",
        "\n",
        "### 3. Model Architecture (Dense Neural Network)\n",
        "*   **Model Class:** A `DiabetesModel` class was defined, inheriting from `nn.Module`.\n",
        "*   **Layers:** The network consists of an input layer, two hidden dense (fully connected) layers, and an output layer:\n",
        "    *   Input Layer: Takes 10 features as input.\n",
        "    *   Hidden Layer 1: `nn.Linear(10, 5)` with 5 neurons and `nn.ReLU()` activation.\n",
        "    *   Hidden Layer 2: `nn.Linear(5, 5)` with 5 neurons and `nn.ReLU()` activation.\n",
        "    *   Output Layer: `nn.Linear(5, 1)` with 1 neuron (for regression) and no activation function.\n",
        "\n",
        "### 4. Model Training\n",
        "*   **Loss Function:** Mean Squared Error (`nn.MSELoss()`) was chosen, appropriate for regression tasks.\n",
        "*   **Optimizer:** The Adam optimizer (`optim.Adam()`) was used to update model weights during training, with a learning rate of 0.001.\n",
        "*   **Training Loop:** The model was trained over a specified number of `epochs` (initially 100, then increased to 1000). Each epoch iterates through the `train_loader`, performs forward pass, calculates loss, backpropagates gradients (`loss.backward()`), and updates weights (`optimiser.step()`).\n",
        "*   **GPU Utilization:** The code checks for GPU availability and moves tensors and the model to the GPU if 'cuda' is available for faster training.\n",
        "\n",
        "### 5. Model Evaluation\n",
        "*   **Evaluation Mode:** The model was set to evaluation mode (`model.eval()`) to disable dropout and batch normalization updates.\n",
        "*   **Metrics:** Average Mean Squared Error (MSE) on the test set was calculated to quantify performance.\n",
        "*   **Prediction Comparison:** Predicted and actual values from the test set were collected and displayed in a `pandas.DataFrame` for visual inspection of the model's performance."
      ]
    }
  ]
}